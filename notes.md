## Author list

* Damien Irving (Melbourne 2015 poster session coordinator, lead author)  
* Kim Doyle (Melbourne 2016 poster session coordinator)  
* Tim Rice (Melbourne 2017 poster session coordinator)  
* Stephanie Bradbury (Brisbane 2016 poster session coordinator)  
* Amanda Miotto (Brisbane 2017 poster session coordinator)  
* Heidi Perrett (Brisbane 2017 poster session coordinator)  
* Belinda Weaver (ResBaz Brisbane coordinator)  
* David Flanders (ResBaz Melbourne coordinator)  

## Introduction

Relevant literature:  
* Around 18% of all researchers use LaTeX [(Pepe, 2016)](https://www.authorea.com/users/3/articles/107393-how-many-scholarly-articles-are-written-in-latex/_show_article)  


## Methodology

The ResBaz poster sessions are explained at [this post](http://melbourne.resbaz.edu.au/post/108054124634/the-resbaz-poster-session-with-a-difference).

Research disciplines were taken from the [Australian and New Zealand Standard Research Classification (ANZSRC), 2008](http://www.abs.gov.au/ausstats/abs@.nsf/Products/6BB427AB9696C225CA2574180004463E?opendocument).

Scope of analysis:  
* Primary interest: Digital tools for organising, analysing and visualising data 
* Side interest: Digital tools that help with research (e.g. referencing, document editing, graphics editing) 
* Out: Complex computer models (e.g. climate models, biological models, economic models)
* Out: Physical hardware (e.g. microscopes, scanners)

Things to look at:
* The full list of tools
* The relative popularity of the tools
  * Split into separate research disciplines
* Tools that cross discipline boundaries

Results to produce:
* Tool list
* Total usage plot:
  * For each tool category produce a bar chart with bars broken into colors for each broad discipline
  * Produce an alternative plot scaled by number of people from each discipline (e.g. the bar chunks would represent the number of users per hypothetical sample of 100 researchers - in other words, had we sampled evenly this is what the results would look like)
  * Perhaps some sub-disciplines are so large and/or distinct that they could form their own group on the plot? (e.g. atmospheric scientists and geologists are very different and probably shouldn't be grouped as earth sciences)
* Discipline usage plot:
  * Same as total usage plot but for each broad discipline (i.e. bars broken down by sub-discipline)


## Discussion / conclusions

There are so many tools that you couldn't possibly provide support and training for all of them.

What we can do with this dataset is identify the most popular tools (i.e. the ones to focus our training efforts on).
When teaching these tools, we need to recognise that researchers use a wide variety of tools
and that the specific tools they use will change over time.
If we can incorporate a strong emphasis on generic skills / best practices into our training,
people walk out with skills in that specific tool *and* 
generic skills that empower them to learn and use other tools more effectively.

The skills / best practices that cover all the tools on the posters can be broadly grouped as follows:
1. Discipline specific knowledge/skills (e.g. MRI analysis software)
2. Research statistics
3. Data visualisation best practices
4. Programming best practices
5. Spreadsheet best practices

Faculties offer (1) and (2) (the latter possibly in collaboration with Maths & Stats Department),
which leaves (3), (4) and (5) to departments like Research Platform Services.
We do programming best practices well (e.g. Software Carpentry),
spreadsheet best practices well (e.g. Data Carpentry),
but there seems to be a gap in what we offer for data visualisation best practices.

The usage patterns of the various digital research tools cross many discipline boundaries.
In other words, researchers from completely separate research fields can often have a lot in common
when it comes to the digital research tools that they use.

The plethora of closed and/or paid and/or no scripting tools is a barrier to open science that doesn't get much attention.

Open (and free) tools typically require coding skills (unlike paid/closed, which often have a GUI).
This means teaching coding best practices empowers researchers to do open science.


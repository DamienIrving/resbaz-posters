## Author list

* Damien Irving (Melbourne 2015 poster session coordinator, lead author)  
* Kim Doyle (Melbourne 2016 poster session coordinator)  
* Tim Rice (Melbourne 2017 poster session coordinator)  
* Stephanie Bradbury (Brisbane 2016 poster session coordinator)  
* Amanda Miotto (Brisbane 2017 poster session coordinator)  
* Heidi Perrett (Brisbane 2017 poster session coordinator)  
* Belinda Weaver (ResBaz Brisbane coordinator)  
* David Flanders (ResBaz Melbourne coordinator)  

## Introduction

It would be of assistance to educators, open science thinkers/advocates and individual researchers to lift the veil
and understand what tools people are using to do their work. 
This information isn't covered in journal papers 
and even within research departments/groups people can be ashamed to talk about the tools they use.

Relevant literature:  
* Around 18% of all researchers use LaTeX [(Pepe, 2016)](https://www.authorea.com/users/3/articles/107393-how-many-scholarly-articles-are-written-in-latex/_show_article)  


## Methodology

The ResBaz poster sessions are explained at [this post](http://melbourne.resbaz.edu.au/post/108054124634/the-resbaz-poster-session-with-a-difference).

Research disciplines were taken from the [Australian and New Zealand Standard Research Classification (ANZSRC), 2008](http://www.abs.gov.au/ausstats/abs@.nsf/Products/6BB427AB9696C225CA2574180004463E?opendocument).

Scope of analysis:  
* Primary interest: Digital tools for collecting, organising, analysing and visualising/presenting data 
* Side interest: Digital tools that help with these data science tasks (e.g. referencing, document editing, graphics editing) 
* Out: Software that generates data (e.g. climate models, biological models, economic models)
* Out: Software for physical hardware (e.g. microscopes, scanners)
* Out: Online platforms for networking (e.g. twitter, LinkedIn, ResearchGate) or personal websites/blogging (e.g. wordpress, blogger)

Things to look at:
* The full list of tools
* The relative popularity of the tools
  * Split into separate research disciplines
* Tools that cross discipline boundaries

Results to produce:
* Tool list
* Total usage plot:
  * For each tool category produce a bar chart with bars broken into colors for each broad discipline
  * Produce an alternative plot scaled by number of people from each discipline (e.g. the bar chunks would represent the number of users per hypothetical sample of 100 researchers - in other words, had we sampled evenly this is what the results would look like)
  * Perhaps some sub-disciplines are so large and/or distinct that they could form their own group on the plot? (e.g. atmospheric scientists and geologists are very different when it comes to digital tool usage and probably shouldn't be grouped as earth sciences)
* Discipline usage plot:
  * Same as total usage plot but for each broad discipline (i.e. bars broken down by sub-discipline)
  * These plots could be used to inform the total usage plot (i.e. in determining whether some broad disciplines need to be broken into categories)


## Results

### Demographics

- X posters
- Career breakdown
- Discipline breakdown

### Digital toolbox

#### Overall

- Programming languages
  - Show list
  - R, Python and MATLAB most popular, in that order
    - Libraries and IDE's of note...

- General data science tools
  - Show list
  - Excel, Prism most popular general purpose  
  - SPSS, Stata for statistics
  - ArcGIS for unstructured spatial data
  - Nvivo for qualitative data
  
  - Vast majority are pay, graphical tools
  
- Support tools
  - Sublime, Notepad++ and Vim most popular code editors
  - Community hasn't settled on favourite survey tool
  - Dropbox and Google Drive most popular for file storage / sharing
  - Version control: 
    - Git / GitHub most popular combination 
    - Comment on percentage of people who code who listed version control
  - Commmunity hasn't settled on favourite mind/concept mapping
  - Evernote popular for note taking
  - Endnote most popular reference manager. Mendeley and Papers next.
  - Document editing: 
    - Word most popular document editor.
    - LaTeX next (various editing platforms) 
    - Scrivener worth a mention
  - Graphics editing: Photoshop, GIMP, Illustrator, Inkscape
  - PowerPoint

#### Discipline profiles

(For any that have at least 5 people)

- genetics / biochemistry and cell biology
- ecology
- zoology
- neurosciences
- public health and health services


## Discussion / conclusions

There are so many tools that you couldn't possibly provide support and training for all of them.

What we can do with this dataset is identify the most popular tools (i.e. the ones to focus our training efforts on).
When teaching these tools, we need to recognise that researchers use a wide variety of tools
and that the specific tools they use will change over time.
If we can incorporate a strong emphasis on generic skills / best practices into our training,
people walk out with skills in that specific tool *and* 
generic skills that empower them to learn and use other tools more effectively.

The skills / best practices that cover all the tools on the posters can be broadly grouped as follows:
1. Discipline specific knowledge/skills (e.g. MRI analysis software)
2. Research statistics
3. Data visualisation best practices
4. Programming best practices
5. Spreadsheet best practices

Faculties offer (1) and (2) (the latter possibly in collaboration with Maths & Stats Department),
which leaves (3), (4) and (5) to departments like Research Platform Services.
We do programming best practices well (e.g. Software Carpentry),
spreadsheet best practices well (e.g. Data Carpentry),
but there seems to be a gap in what we offer for data visualisation best practices.

The usage patterns of the various digital research tools cross many discipline boundaries.
In other words, researchers from completely separate research fields can often have a lot in common
when it comes to the digital research tools that they use.

The plethora of closed and/or paid and/or no scripting tools is a barrier to open science that doesn't get much attention.

Open (and free) tools typically require coding skills (unlike paid/closed, which often have a GUI).
This means teaching coding best practices empowers researchers to do open science.

Further work: A more formal survey of the research community using the results from these open-ended posters 
as a starting point for designing the questions / format.
